{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "from INRIA import epoching, load_MS, test_pipeline\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.estimation import Covariances\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "import shap\n",
    "import pandas\n",
    "mne.set_log_level(verbose=\"Warning\")  # set all the mne verbose to warning\n",
    "\n",
    "seed(2002012)\n",
    "\n",
    "# Deep learning specific parameters\n",
    "input_window_samples = 1024\n",
    "n_epochs = 500\n",
    "n_classes = 2\n",
    "batch_size = 32\n",
    "\n",
    "# dictionary for all our testing pipelines\n",
    "pipelines = {}\n",
    "#pipelines['8csp+lda'] = make_pipeline(CSP(n_components=8), LDA())  # baseline comparison CSP+LDA\n",
    "#pipelines['MDM'] = make_pipeline(Covariances(estimator='lwf'), MDM(metric='riemann', n_jobs=-1))  # simple Riemannian\n",
    "#pipelines['tangentspace+LR'] = make_pipeline(Covariances(estimator='lwf'),\n",
    "#                                             TangentSpace(metric='riemann'),\n",
    "#                                             LogisticRegression(max_iter=350, n_jobs=-1))  # more realistic Riemannian"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [],
   "source": [
    "def SHAP_format(input, labels, columns):\n",
    "    # reformat data into pandas DF that SHAP expects\n",
    "    epochs = input.shape[0]\n",
    "    features = input.shape[1]\n",
    "    samples = input.shape[2]\n",
    "    temp = np.reshape(input, (features, epochs*samples)).transpose((1, 0))\n",
    "    outX = pandas.DataFrame(data=temp, index=range(epochs*samples), columns=columns)\n",
    "    outY = np.repeat(labels, samples)\n",
    "\n",
    "    return outX, outY\n",
    "\n",
    "\n",
    "class SHAPed():\n",
    "    \"\"\"\n",
    "    This is a dummy pipeline step. Its sole purpose is to take SHAP formatted data (features, samples)\n",
    "    and turn it back into MNE formatted data (epochs, features, samples).\n",
    "    \"\"\"\n",
    "    def __init__(self, test):\n",
    "        self.test = test\n",
    "\n",
    "    def undo_SHAP(self, input):\n",
    "        # undo the SHAP_format function, returning data to 3D MNE format\n",
    "        try:  # if input is a pandas df...\n",
    "            input = input.to_numpy()\n",
    "        except AttributeError:\n",
    "            pass  # if it is not, then it's already numpy\n",
    "        dim = input.shape[0]\n",
    "        features = input.shape[1]\n",
    "        sfreq = 256\n",
    "\n",
    "        while sfreq > 0:  # decrement sfreq until we can successfully reshape the data into MNE format\n",
    "            epochs = int(dim/sfreq)\n",
    "            try:\n",
    "                return np.reshape(input.transpose((1,0)), (epochs, features, sfreq))\n",
    "            except:\n",
    "                sfreq = sfreq - 1\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if len(X.shape) == 2:\n",
    "            return self.undo_SHAP(X)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "\n",
    "pipelines['SHAP+tangentspace+LR'] = make_pipeline(SHAPed(test=True),\n",
    "                                                  Covariances(estimator='lwf'),\n",
    "                                                  TangentSpace(metric='riemann'),\n",
    "                                                  LogisticRegression(max_iter=350, n_jobs=-1))  # more realistic Riemannian"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [],
   "source": [
    "def preprocess(raw, steps={}):\n",
    "    print(\"Preprocessing raw data...\")  # for progress tracking\n",
    "    \"\"\" preprocess the data\"\"\"\n",
    "    assert isinstance(steps, dict), \"steps must be a dictionary\"\n",
    "    raw.load_data()\n",
    "    if \"drop_channels\" in steps.keys():\n",
    "        # remove the wanted channels\n",
    "        for channel in steps[\"drop_channels\"]:  # for each channel to be dropped...\n",
    "            if channel in raw.ch_names:  # ensure that it is actually a channel in the data\n",
    "                raw.drop_channels(channel)\n",
    "\n",
    "    if \"filter\" in steps.keys():\n",
    "        assert isinstance(steps[\"filter\"], list), \"filter parameters must be a list in the form [l_freq,h_freq]\"\n",
    "        raw.filter(steps[\"filter\"][0], steps[\"filter\"][1])\n",
    "\n",
    "    return raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [],
   "source": [
    "def epoching(dict, key_session=[], steps_preprocess=None, key_events={\"769\": 0, \"770\": 1}):\n",
    "    print(\"Epoching the data...\")  # for progress tracking\n",
    "    # we are epoching for the RG classifiers\n",
    "    \"\"\"From the dictionary of mne.rawGDF extract all the epochs selected with Key_session\n",
    "     Return the epochs list as X and the label as Y\"\"\"\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    tmin = steps_preprocess[\"tmin\"]\n",
    "    tmax = steps_preprocess[\"tmax\"]\n",
    "    length_epoch = steps_preprocess[\"length\"]\n",
    "    overlap = steps_preprocess[\"overlap\"]\n",
    "    # ---------------------------------------------\n",
    "    X = None\n",
    "    Y = None\n",
    "\n",
    "    for key in key_session:\n",
    "        if steps_preprocess is not None:\n",
    "            _ = preprocess(dict[key], steps_preprocess)\n",
    "\n",
    "        # MNE recommends the following process prior to signal decimation:\n",
    "        current_sfreq = dict[key].info['sfreq']\n",
    "        desired_sfreq = 256  # Hz\n",
    "        decim = np.round(current_sfreq / desired_sfreq).astype(int)\n",
    "        obtained_sfreq = current_sfreq / decim\n",
    "        lowpass_freq = obtained_sfreq / 3.0\n",
    "        dict[key].filter(l_freq=None, h_freq=lowpass_freq, n_jobs=-1)\n",
    "\n",
    "        epoch = mne.Epochs(dict[key], mne.events_from_annotations(dict[key], key_events)[0], tmin=-1, tmax=5,\n",
    "                           baseline=(None, 0), verbose=\"CRITICAL\", decim=decim)[list(key_events.values())]\n",
    "        # NOTE: we are decimating the signal to 256 Hz and only grabbing 2 events to speed up processing\n",
    "\n",
    "        list_start = np.arange(tmin, (tmax + overlap) - length_epoch, overlap)\n",
    "        list_stop = np.arange(tmin + length_epoch, (tmax + overlap), overlap)\n",
    "\n",
    "        for start, stop in zip(list_start, list_stop):\n",
    "            if X is None:\n",
    "                X = epoch.get_data(tmin=start, tmax=stop)\n",
    "                Y = epoch.events[:, 2]\n",
    "\n",
    "            else:\n",
    "                X = np.append(X, epoch.get_data(tmin=start, tmax=stop), axis=0)\n",
    "                Y = np.append(Y, epoch.events[:, 2], axis=0)\n",
    "\n",
    "    return X, Y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [],
   "source": [
    "def test_pipeline(test, pipelines, session, steps_preprocess):\n",
    "    \"\"\" Take in input the different pipelines to test and return the corresponding classification accuracy\"\"\"\n",
    "    accuracy = pd.DataFrame(np.zeros((len(session), len(pipelines))), index=session, columns=pipelines.keys())\n",
    "\n",
    "    for subject in session:  # this will be something like \"A10\" for MS and \"S06_S1\" for SS\n",
    "        print(\"Running a leave-one-out classification, leaving out every MS subject one at a time...\")  # for progress tracking\n",
    "        train_key = {k: v for k, v in test.items() if subject not in k}  # train on all subjects data EXCEPT one\n",
    "        test_key = {k: v for k, v in test.items() if subject in k}  # test on that subject\n",
    "\n",
    "        X_train, Y_train = epoching(test, train_key, steps_preprocess)  # X = epochs, Y = labels\n",
    "        X_test, Y_test = epoching(test, test_key, steps_preprocess)  # same as above, but test set\n",
    "\n",
    "        for classifier in pipelines.keys():\n",
    "            channels = ['Fz', 'FCz', 'Cz', 'CPz', 'Pz', 'C1', 'C3', 'C5', 'C2', 'C4',\n",
    "            'C6', 'F4', 'FC2', 'FC4', 'FC6', 'CP2', 'CP4', 'CP6', 'P4',\n",
    "            'F3', 'FC1', 'FC3', 'FC5', 'CP1', 'CP3', 'CP5', 'P3']\n",
    "            newTrain, _ = SHAP_format(X_train, Y_train, channels)\n",
    "            print(\"Fitting the classifier...\")\n",
    "            pipelines[classifier].fit(newTrain, Y_train)\n",
    "\n",
    "            if steps_preprocess[\"score\"] == \"TAcc\":\n",
    "\n",
    "                # ---------------------------------------------\n",
    "                tmin = steps_preprocess[\"tmin\"]\n",
    "                tmax = steps_preprocess[\"tmax\"]\n",
    "                length_epoch = steps_preprocess[\"length\"]\n",
    "                overlap = steps_preprocess[\"overlap\"]\n",
    "                # ---------------------------------------------\n",
    "                dist = len(np.arange(tmin, (tmax + overlap) - length_epoch, overlap))\n",
    "\n",
    "                X_estim = pipelines[classifier].transform(X_test)\n",
    "\n",
    "                X_estim_reshape = X_estim.reshape((-1, dist))\n",
    "                X_sum = X_estim_reshape.sum(axis=0)\n",
    "\n",
    "                trial_predict = np.where(X_sum < 0, 0, 1)  # if the sum < 0, left. If >0, predict right\n",
    "                temporary_accuracy = np.where(trial_predict == Y_test[0::dist - 1], 1, 0)  # Compare predictions with observations\n",
    "\n",
    "                accuracy[classifier][subject] = temporary_accuracy.mean()\n",
    "\n",
    "            elif steps_preprocess[\"score\"] == \"EAcc\":\n",
    "                try:\n",
    "                    accuracy[classifier][subject] = pipelines[classifier].score(X_test, Y_test)\n",
    "                except:\n",
    "                    accuracy[classifier][subject] = np.nan\n",
    "            elif steps_preprocess[\"score\"] == \"SHAP\":\n",
    "                print(\"Caclulating SHAP values...\")\n",
    "                f = lambda x: pipelines[classifier].predict(x)  # establish our prediction function\n",
    "                explainer = shap.KernelExplainer(f, X_train)  # declare our explainer\n",
    "\n",
    "                newTest, _ = SHAP_format(X_test, Y_test, channels)\n",
    "                explainme = newTest.iloc[0:255, :]\n",
    "                shap_values = explainer.shap_values(explainme, nsamples=1000)  # calculate SHAP values for the given range\n",
    "                exp = shap.Explanation(shap_values, explainer.expected_value, data=explainme.values, feature_names=channels)  # explain these values\n",
    "                shap.waterfall_plot(exp)  # waterfall plot a random sample explanation\n",
    "\n",
    "                accuracy[classifier][subject] = shap_values\n",
    "            else:\n",
    "                raise AttributeError(\"The chosen score does not exist!\")\n",
    "\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [],
   "source": [
    "def load_MS(between=False, within=False):\n",
    "    print(\"Loading the MS dataset (all subjects that are available)...\")  # for progress tracking\n",
    "    # Establish master data path\n",
    "    path = os.path.join(\"Data\", \"MS\")\n",
    "\n",
    "    # Load all files for a given range of participants\n",
    "    # Format: [SUB]_[RUN_NAME].gdf\n",
    "\n",
    "    runNames = [\"CE_baseline\", \"OE_baseline\", \"R1_acquisition\", \"R2_acquisition\",\n",
    "                \"R3_onlineT\", \"R4_onlineT\", \"R5_onlineT\",\n",
    "                \"R6_onlineT\"]  # run IDs: CE/OE = closed/open eyes, RX = run #X\n",
    "    subA = [\"A\" + str(i) for i in range(1, 61)]  # 60 participants (Dataset A ; 29 women ; age 19-59, M = 29, SD = 9.32)\n",
    "    subB = [\"B\" + str(i) for i in\n",
    "            range(61, 82)]  # 21 participants (Dataset B ; 8 women ; age 19-37, M = 29, SD = 9.318)\n",
    "    subC = [\"C\" + str(i) for i in\n",
    "            range(82, 88)]  # 6 additional participants (Dataset C; 4 women; age 20-26, M=22; SD=2.34)\n",
    "    subjects = subA + subB + subC\n",
    "    # Note that all data followed the same EEG protocol (MI of left and right hand) with the same channel information\n",
    "\n",
    "    data = {}  # dictionary to hold all our data\n",
    "    for sub in subjects:  # for each subject...\n",
    "        skip = False\n",
    "        fnames = [sub + \"_\" + i + \".gdf\" for i in runNames]  # develop a list of all filenames\n",
    "        sub_data = []  # empty list to hold all of a subject's loaded files\n",
    "        for f in fnames:  # for each file...\n",
    "            fpath = os.path.join(path, sub, f)  # select the correct file\n",
    "            try:\n",
    "                sub_data.append(mne.io.read_raw_gdf(fpath))  # load it into the list\n",
    "            except FileNotFoundError:\n",
    "                skip = True  # if all GDF files are not available, skip this subject (for testing purposes only)\n",
    "        if not skip:\n",
    "            # correct channel type information (to properly label EOG and EMG channels)\n",
    "            new_types = []  # create a new channel types array\n",
    "            for i in sub_data:\n",
    "                for j in i.ch_names:\n",
    "                    if \"EOG\" in j:  # mark ECOG channels\n",
    "                        new_types.append(\"ecog\")\n",
    "                    elif \"EMG\" in j:  # mark EMG channels\n",
    "                        new_types.append(\"emg\")\n",
    "                    else:  # mark the rest as regular EEG\n",
    "                        new_types.append(\"eeg\")\n",
    "                i.set_channel_types(dict(zip(i.ch_names, new_types)))  # apply new channel types to raw object\n",
    "\n",
    "            data[sub] = sub_data  # save sub_data list into data dictionary\n",
    "\n",
    "    # Current data format: data[subject] holds all 8 raw objects\n",
    "    # data[subject][0] = Closed eyes baseline\n",
    "    # data[subject][1] = Open eyes baseline\n",
    "    # data[subject][2] = Training session 1\n",
    "    # data[subject][3] = Training session 2\n",
    "    # data[subject][4] = Test session 1 (w/ feedback)\n",
    "    # data[subject][5] = Test session 2 (w/ feedback)\n",
    "    # data[subject][6] = Test session 3 (w/ feedback)\n",
    "    # data[subject][7] = Test session 4 (w/ feedback)\n",
    "\n",
    "    if between:\n",
    "        # dic_data_format = participant number (+ _N for train) (+ _N++ for test) : mne raw object\n",
    "        dic_data = {}\n",
    "        for i in data.keys():  # for every subject...\n",
    "            for j in range(1, 7):\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data[session] = data[i][j + 1]  # place their sessions into one dictionary following indexes above\n",
    "        return data, dic_data\n",
    "    if within:\n",
    "        dic_data_train = {}\n",
    "        dic_data_test = {}\n",
    "        for i in data.keys():  # for every subject...\n",
    "            for j in range(1, 3):  # place their training sessions into one dictionary\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data_train[session] = data[i][j + 1]  # following the indexes from the comment above\n",
    "            for j in range(3, 7):  # and their testing sessions into another dictionary\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data_test[session] = data[i][j + 1]  # with the same indexing as the comment block above\n",
    "        return data, dic_data_train, dic_data_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [],
   "source": [
    "def MS_RG_Between():\n",
    "    # load the MS dataset\n",
    "    data, dic_data = load_MS(between=True)\n",
    "\n",
    "    # run the selected pipelines\n",
    "    session = list(data.keys())  # a list of participants to be used for analysis\n",
    "    steps_preprocess = {\"filter\": [8, 30],  # filter from 8-30Hz\n",
    "                        \"drop_channels\": ['EOG1', 'EOG2', 'EOG3', 'EMGg', 'EMGd'],  # ignore EOG/EMG channels\n",
    "                        \"tmin\": 0.5, \"tmax\": 4, \"overlap\": 1/16, \"length\": 1,\n",
    "                        \"score\": \"SHAP\"}\n",
    "    accuracy = test_pipeline(dic_data, pipelines, session, steps_preprocess)  # run it!\n",
    "\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MS dataset (all subjects that are available)...\n",
      "Running a leave-one-out classification, leaving out every MS subject one at a time...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Fitting the classifier...\n",
      "Caclulating SHAP values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 984 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/255 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ad25d1c7f7f490b8347092f81fcff37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only one sample available. You may want to reshape your data array\n",
      "Only one sample available. You may want to reshape your data array\n",
      "divide by zero encountered in log\n",
      "invalid value encountered in multiply\n",
      "invalid value encountered in matmul\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [322], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mMS_RG_Between\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [321], line 11\u001B[0m, in \u001B[0;36mMS_RG_Between\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data\u001B[38;5;241m.\u001B[39mkeys())  \u001B[38;5;66;03m# a list of participants to be used for analysis\u001B[39;00m\n\u001B[0;32m      7\u001B[0m steps_preprocess \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilter\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m30\u001B[39m],  \u001B[38;5;66;03m# filter from 8-30Hz\u001B[39;00m\n\u001B[0;32m      8\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrop_channels\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEOG1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEOG2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEOG3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEMGg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEMGd\u001B[39m\u001B[38;5;124m'\u001B[39m],  \u001B[38;5;66;03m# ignore EOG/EMG channels\u001B[39;00m\n\u001B[0;32m      9\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtmin\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtmax\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m4\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverlap\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m16\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     10\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSHAP\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m---> 11\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtest_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdic_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpipelines\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps_preprocess\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# run it!\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m accuracy\n",
      "Cell \u001B[1;32mIn [319], line 53\u001B[0m, in \u001B[0;36mtest_pipeline\u001B[1;34m(test, pipelines, session, steps_preprocess)\u001B[0m\n\u001B[0;32m     51\u001B[0m newTest, _ \u001B[38;5;241m=\u001B[39m SHAP_format(X_test, Y_test, channels)\n\u001B[0;32m     52\u001B[0m explainme \u001B[38;5;241m=\u001B[39m newTest\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m255\u001B[39m, :]\n\u001B[1;32m---> 53\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m \u001B[43mexplainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshap_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexplainme\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnsamples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# calculate SHAP values for the given range\u001B[39;00m\n\u001B[0;32m     54\u001B[0m exp \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplanation(shap_values, explainer\u001B[38;5;241m.\u001B[39mexpected_value, data\u001B[38;5;241m=\u001B[39mexplainme\u001B[38;5;241m.\u001B[39mvalues, feature_names\u001B[38;5;241m=\u001B[39mchannels)  \u001B[38;5;66;03m# explain these values\u001B[39;00m\n\u001B[0;32m     55\u001B[0m shap\u001B[38;5;241m.\u001B[39mwaterfall_plot(exp)  \u001B[38;5;66;03m# waterfall plot a random sample explanation\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\shap\\explainers\\_kernel.py:190\u001B[0m, in \u001B[0;36mKernel.shap_values\u001B[1;34m(self, X, **kwargs)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_index:\n\u001B[0;32m    189\u001B[0m     data \u001B[38;5;241m=\u001B[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m], index_name)\n\u001B[1;32m--> 190\u001B[0m explanations\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplain(data, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgc_collect\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    192\u001B[0m     gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\shap\\explainers\\_kernel.py:236\u001B[0m, in \u001B[0;36mKernel.explain\u001B[1;34m(self, incoming_instance, **kwargs)\u001B[0m\n\u001B[0;32m    234\u001B[0m     model_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mf(instance\u001B[38;5;241m.\u001B[39mconvert_to_df())\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 236\u001B[0m     model_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model_out, (pd\u001B[38;5;241m.\u001B[39mDataFrame, pd\u001B[38;5;241m.\u001B[39mSeries)):\n\u001B[0;32m    238\u001B[0m     model_out \u001B[38;5;241m=\u001B[39m model_out\u001B[38;5;241m.\u001B[39mvalues\n",
      "Cell \u001B[1;32mIn [319], line 48\u001B[0m, in \u001B[0;36mtest_pipeline.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m steps_preprocess[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSHAP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCaclulating SHAP values...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 48\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mpipelines\u001B[49m\u001B[43m[\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# establish our prediction function\u001B[39;00m\n\u001B[0;32m     49\u001B[0m     explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mKernelExplainer(f, X_train)  \u001B[38;5;66;03m# declare our explainer\u001B[39;00m\n\u001B[0;32m     51\u001B[0m     newTest, _ \u001B[38;5;241m=\u001B[39m SHAP_format(X_test, Y_test, channels)\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\sklearn\\pipeline.py:458\u001B[0m, in \u001B[0;36mPipeline.predict\u001B[1;34m(self, X, **predict_params)\u001B[0m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, name, transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(with_final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    457\u001B[0m     Xt \u001B[38;5;241m=\u001B[39m transform\u001B[38;5;241m.\u001B[39mtransform(Xt)\n\u001B[1;32m--> 458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mpredict(Xt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpredict_params)\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:447\u001B[0m, in \u001B[0;36mLinearClassifierMixin.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;124;03m    Predict class labels for samples in X.\u001B[39;00m\n\u001B[0;32m    436\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;124;03m        Vector containing the class labels for each sample.\u001B[39;00m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 447\u001B[0m     scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecision_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    448\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(scores\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    449\u001B[0m         indices \u001B[38;5;241m=\u001B[39m (scores \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:429\u001B[0m, in \u001B[0;36mLinearClassifierMixin.decision_function\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    409\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    410\u001B[0m \u001B[38;5;124;03mPredict confidence scores for samples.\u001B[39;00m\n\u001B[0;32m    411\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;124;03m    this class would be predicted.\u001B[39;00m\n\u001B[0;32m    426\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    427\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 429\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    430\u001B[0m scores \u001B[38;5;241m=\u001B[39m safe_sparse_dot(X, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoef_\u001B[38;5;241m.\u001B[39mT, dense_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept_\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m scores\u001B[38;5;241m.\u001B[39mravel() \u001B[38;5;28;01mif\u001B[39;00m scores\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m scores\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\sklearn\\base.py:577\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 577\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    578\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    893\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    894\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    895\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[0;32m    896\u001B[0m         )\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[1;32m--> 899\u001B[0m         \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    900\u001B[0m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    901\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    902\u001B[0m \u001B[43m            \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    903\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    904\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    907\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m allow_nan\n\u001B[0;32m    126\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m estimator_name\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[0;32m    131\u001B[0m             \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m             msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    133\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    144\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    145\u001B[0m             )\n\u001B[1;32m--> 146\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n",
      "\u001B[1;31mValueError\u001B[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "accuracy = MS_RG_Between()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
