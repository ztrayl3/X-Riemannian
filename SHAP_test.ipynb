{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "from INRIA import epoching, load_MS, test_pipeline\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.estimation import Covariances\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "import shap\n",
    "mne.set_log_level(verbose=\"Warning\")  # set all the mne verbose to warning\n",
    "\n",
    "seed(2002012)\n",
    "\n",
    "# Deep learning specific parameters\n",
    "input_window_samples = 1024\n",
    "n_epochs = 500\n",
    "n_classes = 2\n",
    "batch_size = 32\n",
    "\n",
    "# dictionary for all our testing pipelines\n",
    "pipelines = {}\n",
    "pipelines['8csp+lda'] = make_pipeline(CSP(n_components=8), LDA())  # baseline comparison CSP+LDA\n",
    "pipelines['MDM'] = make_pipeline(Covariances(estimator='lwf'), MDM(metric='riemann', n_jobs=-1))  # simple Riemannian\n",
    "pipelines['tangentspace+LR'] = make_pipeline(Covariances(estimator='lwf'),\n",
    "                                             TangentSpace(metric='riemann'),\n",
    "                                             LogisticRegression(max_iter=350, n_jobs=-1))  # more realistic Riemannian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "def preprocess(raw, steps={}):\n",
    "    print(\"Preprocessing raw data...\")  # for progress tracking\n",
    "    \"\"\" preprocess the data\"\"\"\n",
    "    assert isinstance(steps, dict), \"steps must be a dictionary\"\n",
    "    raw.load_data()\n",
    "    if \"drop_channels\" in steps.keys():\n",
    "        # remove the wanted channels\n",
    "        for channel in steps[\"drop_channels\"]:  # for each channel to be dropped...\n",
    "            if channel in raw.ch_names:  # ensure that it is actually a channel in the data\n",
    "                raw.drop_channels(channel)\n",
    "\n",
    "    if \"filter\" in steps.keys():\n",
    "        assert isinstance(steps[\"filter\"], list), \"filter parameters must be a list in the form [l_freq,h_freq]\"\n",
    "        raw.filter(steps[\"filter\"][0], steps[\"filter\"][1])\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "def epoching(dict, key_session=[], steps_preprocess=None, key_events={\"769\": 0, \"770\": 1}):\n",
    "    print(\"Epoching the data...\")  # for progress tracking\n",
    "    # we are epoching for the RG classifiers\n",
    "    \"\"\"From the dictionary of mne.rawGDF extract all the epochs selected with Key_session\n",
    "     Return the epochs list as X and the label as Y\"\"\"\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    tmin = steps_preprocess[\"tmin\"]\n",
    "    tmax = steps_preprocess[\"tmax\"]\n",
    "    length_epoch = steps_preprocess[\"length\"]\n",
    "    overlap = steps_preprocess[\"overlap\"]\n",
    "    # ---------------------------------------------\n",
    "    X = None\n",
    "    Y = None\n",
    "\n",
    "    for key in key_session:\n",
    "        if steps_preprocess is not None:\n",
    "            _ = preprocess(dict[key], steps_preprocess)\n",
    "\n",
    "        # MNE recommends the following process prior to signal decimation:\n",
    "        current_sfreq = dict[key].info['sfreq']\n",
    "        desired_sfreq = 256  # Hz\n",
    "        decim = np.round(current_sfreq / desired_sfreq).astype(int)\n",
    "        obtained_sfreq = current_sfreq / decim\n",
    "        lowpass_freq = obtained_sfreq / 3.0\n",
    "        dict[key].filter(l_freq=None, h_freq=lowpass_freq, n_jobs=-1)\n",
    "\n",
    "        epoch = mne.Epochs(dict[key], mne.events_from_annotations(dict[key], key_events)[0], tmin=-1, tmax=5,\n",
    "                           baseline=(None, 0), verbose=\"CRITICAL\", decim=decim)[list(key_events.values())]\n",
    "        # NOTE: we are decimating the signal to 256 Hz and only grabbing 2 events to speed up processing\n",
    "\n",
    "        list_start = np.arange(tmin, (tmax + overlap) - length_epoch, overlap)\n",
    "        list_stop = np.arange(tmin + length_epoch, (tmax + overlap), overlap)\n",
    "\n",
    "        for start, stop in zip(list_start, list_stop):\n",
    "            if X is None:\n",
    "                X = epoch.get_data(tmin=start, tmax=stop)\n",
    "                Y = epoch.events[:, 2]\n",
    "\n",
    "            else:\n",
    "                X = np.append(X, epoch.get_data(tmin=start, tmax=stop), axis=0)\n",
    "                Y = np.append(Y, epoch.events[:, 2], axis=0)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def test_pipeline(test, pipelines, session, steps_preprocess):\n",
    "    \"\"\" Take in input the different pipelines to test and return the corresponding classification accuracy\"\"\"\n",
    "    accuracy = pd.DataFrame(np.zeros((len(session), len(pipelines))), index=session, columns=pipelines.keys())\n",
    "\n",
    "    for subject in session:  # this will be something like \"A10\" for MS and \"S06_S1\" for SS\n",
    "        print(\"Running a leave-one-out classification, leaving out every MS subject one at a time...\")  # for progress tracking\n",
    "        train_key = {k: v for k, v in test.items() if subject not in k}  # train on all subjects data EXCEPT one\n",
    "        test_key = {k: v for k, v in test.items() if subject in k}  # test on that subject\n",
    "\n",
    "        X_train, Y_train = epoching(test, train_key, steps_preprocess)  # X = epochs, Y = labels\n",
    "        X_test, Y_test = epoching(test, test_key, steps_preprocess)  # same as above, but test set\n",
    "\n",
    "        for classifier in pipelines.keys():\n",
    "            pipelines[classifier].fit(X_train, Y_train)\n",
    "\n",
    "            if steps_preprocess[\"score\"] == \"TAcc\":\n",
    "\n",
    "                # ---------------------------------------------\n",
    "                tmin = steps_preprocess[\"tmin\"]\n",
    "                tmax = steps_preprocess[\"tmax\"]\n",
    "                length_epoch = steps_preprocess[\"length\"]\n",
    "                overlap = steps_preprocess[\"overlap\"]\n",
    "                # ---------------------------------------------\n",
    "                dist = len(np.arange(tmin, (tmax + overlap) - length_epoch, overlap))\n",
    "\n",
    "                X_estim = pipelines[classifier].transform(X_test)\n",
    "\n",
    "                X_estim_reshape = X_estim.reshape((-1, dist))\n",
    "                X_sum = X_estim_reshape.sum(axis=0)\n",
    "\n",
    "                trial_predict = np.where(X_sum < 0, 0, 1)  # if the sum < 0, left. If >0, predict right\n",
    "                temporary_accuracy = np.where(trial_predict == Y_test[0::dist - 1], 1,\n",
    "                                              0)  # Compare predictions with observations\n",
    "\n",
    "                accuracy[classifier][subject] = temporary_accuracy.mean()\n",
    "\n",
    "            elif steps_preprocess[\"score\"] == \"EAcc\":\n",
    "                try:\n",
    "                    accuracy[classifier][subject] = pipelines[classifier].score(X_test, Y_test)\n",
    "                except:\n",
    "                    accuracy[classifier][subject] = np.nan\n",
    "\n",
    "            elif steps_preprocess[\"score\"] == \"SHAP\":\n",
    "                shap_kernel_explainer = shap.KernelExplainer(pipelines[classifier].predict, X_train, link='logit')\n",
    "                test = X_test.reshape(27, X_test.shape[0]*X_test.shape[2])\n",
    "                shap_values_single = shap_kernel_explainer.shap_values(X_test)\n",
    "                accuracy[classifier][subject] = shap_values_single\n",
    "\n",
    "            else:\n",
    "                raise AttributeError(\"The chosen score does not exist!\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def load_MS(between=False, within=False):\n",
    "    print(\"Loading the MS dataset (all subjects that are available)...\")  # for progress tracking\n",
    "    # Establish master data path\n",
    "    path = os.path.join(\"Data\", \"MS\")\n",
    "\n",
    "    # Load all files for a given range of participants\n",
    "    # Format: [SUB]_[RUN_NAME].gdf\n",
    "\n",
    "    runNames = [\"CE_baseline\", \"OE_baseline\", \"R1_acquisition\", \"R2_acquisition\",\n",
    "                \"R3_onlineT\", \"R4_onlineT\", \"R5_onlineT\",\n",
    "                \"R6_onlineT\"]  # run IDs: CE/OE = closed/open eyes, RX = run #X\n",
    "    subA = [\"A\" + str(i) for i in range(1, 61)]  # 60 participants (Dataset A ; 29 women ; age 19-59, M = 29, SD = 9.32)\n",
    "    subB = [\"B\" + str(i) for i in\n",
    "            range(61, 82)]  # 21 participants (Dataset B ; 8 women ; age 19-37, M = 29, SD = 9.318)\n",
    "    subC = [\"C\" + str(i) for i in\n",
    "            range(82, 88)]  # 6 additional participants (Dataset C; 4 women; age 20-26, M=22; SD=2.34)\n",
    "    subjects = subA + subB + subC\n",
    "    # Note that all data followed the same EEG protocol (MI of left and right hand) with the same channel information\n",
    "\n",
    "    data = {}  # dictionary to hold all our data\n",
    "    for sub in subjects:  # for each subject...\n",
    "        skip = False\n",
    "        fnames = [sub + \"_\" + i + \".gdf\" for i in runNames]  # develop a list of all filenames\n",
    "        sub_data = []  # empty list to hold all of a subject's loaded files\n",
    "        for f in fnames:  # for each file...\n",
    "            fpath = os.path.join(path, sub, f)  # select the correct file\n",
    "            try:\n",
    "                sub_data.append(mne.io.read_raw_gdf(fpath))  # load it into the list\n",
    "            except FileNotFoundError:\n",
    "                skip = True  # if all GDF files are not available, skip this subject (for testing purposes only)\n",
    "        if not skip:\n",
    "            # correct channel type information (to properly label EOG and EMG channels)\n",
    "            new_types = []  # create a new channel types array\n",
    "            for i in sub_data:\n",
    "                for j in i.ch_names:\n",
    "                    if \"EOG\" in j:  # mark ECOG channels\n",
    "                        new_types.append(\"ecog\")\n",
    "                    elif \"EMG\" in j:  # mark EMG channels\n",
    "                        new_types.append(\"emg\")\n",
    "                    else:  # mark the rest as regular EEG\n",
    "                        new_types.append(\"eeg\")\n",
    "                i.set_channel_types(dict(zip(i.ch_names, new_types)))  # apply new channel types to raw object\n",
    "            data[sub] = sub_data  # save sub_data list into data dictionary\n",
    "\n",
    "    # Current data format: data[subject] holds all 8 raw objects\n",
    "    # data[subject][0] = Closed eyes baseline\n",
    "    # data[subject][1] = Open eyes baseline\n",
    "    # data[subject][2] = Training session 1\n",
    "    # data[subject][3] = Training session 2\n",
    "    # data[subject][4] = Test session 1 (w/ feedback)\n",
    "    # data[subject][5] = Test session 2 (w/ feedback)\n",
    "    # data[subject][6] = Test session 3 (w/ feedback)\n",
    "    # data[subject][7] = Test session 4 (w/ feedback)\n",
    "\n",
    "    if between:\n",
    "        # dic_data_format = participant number (+ _N for train) (+ _N++ for test) : mne raw object\n",
    "        dic_data = {}\n",
    "        for i in data.keys():  # for every subject...\n",
    "            for j in range(1, 7):\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data[session] = data[i][j + 1]  # place their sessions into one dictionary following indexes above\n",
    "        return data, dic_data\n",
    "    if within:\n",
    "        dic_data_train = {}\n",
    "        dic_data_test = {}\n",
    "        for i in data.keys():  # for every subject...\n",
    "            for j in range(1, 3):  # place their training sessions into one dictionary\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data_train[session] = data[i][j + 1]  # following the indexes from the comment above\n",
    "            for j in range(3, 7):  # and their testing sessions into another dictionary\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data_test[session] = data[i][j + 1]  # with the same indexing as the comment block above\n",
    "        return data, dic_data_train, dic_data_test\n",
    "\n",
    "\n",
    "def MS_RG_Between():\n",
    "    # load the MS dataset\n",
    "    data, dic_data = load_MS(between=True)\n",
    "\n",
    "    # run the selected pipelines\n",
    "    session = list(data.keys())  # a list of participants to be used for analysis\n",
    "    steps_preprocess = {\"filter\": [8, 30],  # filter from 8-30Hz\n",
    "                        \"drop_channels\": ['EOG1', 'EOG2', 'EOG3', 'EMGg', 'EMGd'],  # ignore EOG/EMG channels\n",
    "                        \"tmin\": 0.5, \"tmax\": 4, \"overlap\": 1/16, \"length\": 1,\n",
    "                        \"score\": \"EAcc\"}\n",
    "    accuracy = test_pipeline(dic_data, pipelines, session, steps_preprocess)  # run it!\n",
    "\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MS dataset (all subjects that are available)...\n",
      "Running a leave-one-out classification, leaving out every MS subject one at a time...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Running a leave-one-out classification, leaving out every MS subject one at a time...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Running a leave-one-out classification, leaving out every MS subject one at a time...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n"
     ]
    }
   ],
   "source": [
    "MS_RG_Between()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}