{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from INRIA import epoching, load_MS, test_pipeline\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.estimation import Covariances\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "import shap\n",
    "mne.set_log_level(verbose=\"Warning\")  # set all the mne verbose to warning\n",
    "\n",
    "seed(2002012)\n",
    "\n",
    "# Deep learning specific parameters\n",
    "input_window_samples = 1024\n",
    "n_epochs = 500\n",
    "n_classes = 2\n",
    "batch_size = 32\n",
    "\n",
    "# dictionary for all our testing pipelines\n",
    "pipelines = {}\n",
    "pipelines['8csp+lda'] = make_pipeline(CSP(n_components=8), LDA())  # baseline comparison CSP+LDA\n",
    "#pipelines['MDM'] = make_pipeline(Covariances(estimator='lwf'), MDM(metric='riemann', n_jobs=-1))  # simple Riemannian\n",
    "#pipelines['tangentspace+LR'] = make_pipeline(Covariances(estimator='lwf'),\n",
    "#                                             TangentSpace(metric='riemann'),\n",
    "#                                             LogisticRegression(max_iter=350, n_jobs=-1))  # more realistic Riemannian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def preprocess(raw, steps={}):\n",
    "    print(\"Preprocessing raw data...\")  # for progress tracking\n",
    "    \"\"\" preprocess the data\"\"\"\n",
    "    assert isinstance(steps, dict), \"steps must be a dictionary\"\n",
    "    raw.load_data()\n",
    "    if \"drop_channels\" in steps.keys():\n",
    "        # remove the wanted channels\n",
    "        for channel in steps[\"drop_channels\"]:  # for each channel to be dropped...\n",
    "            if channel in raw.ch_names:  # ensure that it is actually a channel in the data\n",
    "                raw.drop_channels(channel)\n",
    "\n",
    "    if \"filter\" in steps.keys():\n",
    "        assert isinstance(steps[\"filter\"], list), \"filter parameters must be a list in the form [l_freq,h_freq]\"\n",
    "        raw.filter(steps[\"filter\"][0], steps[\"filter\"][1])\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "def epoching(dict, key_session=[], steps_preprocess=None, key_events={\"769\": 0, \"770\": 1}):\n",
    "    print(\"Epoching the data...\")  # for progress tracking\n",
    "    # we are epoching for the RG classifiers\n",
    "    \"\"\"From the dictionary of mne.rawGDF extract all the epochs selected with Key_session\n",
    "     Return the epochs list as X and the label as Y\"\"\"\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    tmin = steps_preprocess[\"tmin\"]\n",
    "    tmax = steps_preprocess[\"tmax\"]\n",
    "    length_epoch = steps_preprocess[\"length\"]\n",
    "    overlap = steps_preprocess[\"overlap\"]\n",
    "    # ---------------------------------------------\n",
    "    X = None\n",
    "    Y = None\n",
    "\n",
    "    for key in key_session:\n",
    "        if steps_preprocess is not None:\n",
    "            _ = preprocess(dict[key], steps_preprocess)\n",
    "\n",
    "        # MNE recommends the following process prior to signal decimation:\n",
    "        current_sfreq = dict[key].info['sfreq']\n",
    "        desired_sfreq = 256  # Hz\n",
    "        decim = np.round(current_sfreq / desired_sfreq).astype(int)\n",
    "        obtained_sfreq = current_sfreq / decim\n",
    "        lowpass_freq = obtained_sfreq / 3.0\n",
    "        dict[key].filter(l_freq=None, h_freq=lowpass_freq, n_jobs=-1)\n",
    "\n",
    "        epoch = mne.Epochs(dict[key], mne.events_from_annotations(dict[key], key_events)[0], tmin=-1, tmax=5,\n",
    "                           baseline=(None, 0), verbose=\"CRITICAL\", decim=decim)[list(key_events.values())]\n",
    "        # NOTE: we are decimating the signal to 256 Hz and only grabbing 2 events to speed up processing\n",
    "\n",
    "        list_start = np.arange(tmin, (tmax + overlap) - length_epoch, overlap)\n",
    "        list_stop = np.arange(tmin + length_epoch, (tmax + overlap), overlap)\n",
    "\n",
    "        for start, stop in zip(list_start, list_stop):\n",
    "            if X is None:\n",
    "                X = epoch.get_data(tmin=start, tmax=stop)\n",
    "                Y = epoch.events[:, 2]\n",
    "\n",
    "            else:\n",
    "                X = np.append(X, epoch.get_data(tmin=start, tmax=stop), axis=0)\n",
    "                Y = np.append(Y, epoch.events[:, 2], axis=0)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def test_pipeline(test, pipelines, session, steps_preprocess):\n",
    "    \"\"\" Take in input the different pipelines to test and return the corresponding classification accuracy\"\"\"\n",
    "    accuracy = pd.DataFrame(np.zeros((len(session), len(pipelines))), index=session, columns=pipelines.keys())\n",
    "\n",
    "    for subject in session:  # this will be something like \"A10\" for MS and \"S06_S1\" for SS\n",
    "        print(\"Running a leave-one-out classification, leaving out every MS subject one at a time...\")  # for progress tracking\n",
    "        train_key = {k: v for k, v in test.items() if subject not in k}  # train on all subjects data EXCEPT one\n",
    "        test_key = {k: v for k, v in test.items() if subject in k}  # test on that subject\n",
    "\n",
    "        X_train, Y_train = epoching(test, train_key, steps_preprocess)  # X = epochs, Y = labels\n",
    "        X_test, Y_test = epoching(test, test_key, steps_preprocess)  # same as above, but test set\n",
    "\n",
    "        for classifier in pipelines.keys():\n",
    "            pipelines[classifier].fit(X_train, Y_train)\n",
    "\n",
    "            if steps_preprocess[\"score\"] == \"TAcc\":\n",
    "\n",
    "                # ---------------------------------------------\n",
    "                tmin = steps_preprocess[\"tmin\"]\n",
    "                tmax = steps_preprocess[\"tmax\"]\n",
    "                length_epoch = steps_preprocess[\"length\"]\n",
    "                overlap = steps_preprocess[\"overlap\"]\n",
    "                # ---------------------------------------------\n",
    "                dist = len(np.arange(tmin, (tmax + overlap) - length_epoch, overlap))\n",
    "\n",
    "                X_estim = pipelines[classifier].transform(X_test)\n",
    "\n",
    "                X_estim_reshape = X_estim.reshape((-1, dist))\n",
    "                X_sum = X_estim_reshape.sum(axis=0)\n",
    "\n",
    "                trial_predict = np.where(X_sum < 0, 0, 1)  # if the sum < 0, left. If >0, predict right\n",
    "                temporary_accuracy = np.where(trial_predict == Y_test[0::dist - 1], 1,\n",
    "                                              0)  # Compare predictions with observations\n",
    "\n",
    "                accuracy[classifier][subject] = temporary_accuracy.mean()\n",
    "\n",
    "            elif steps_preprocess[\"score\"] == \"EAcc\":\n",
    "                try:\n",
    "                    accuracy[classifier][subject] = pipelines[classifier].score(X_test, Y_test)\n",
    "                except:\n",
    "                    accuracy[classifier][subject] = np.nan\n",
    "            elif steps_preprocess[\"score\"] == \"SHAP\":\n",
    "                explainer = shap.KernelExplainer(pipelines[classifier].predict_proba, X_train, link=\"logit\")\n",
    "                shap_values_single = explainer.shap_values(X_test, nsamples = 100)\n",
    "                accuracy[classifier][subject] = shap_values_single\n",
    "            else:\n",
    "                raise AttributeError(\"The chosen score does not exist!\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def load_MS(between=False, within=False):\n",
    "    print(\"Loading the MS dataset (all subjects that are available)...\")  # for progress tracking\n",
    "    # Establish master data path\n",
    "    path = os.path.join(\"Data\", \"MS\")\n",
    "\n",
    "    # Load all files for a given range of participants\n",
    "    # Format: [SUB]_[RUN_NAME].gdf\n",
    "\n",
    "    runNames = [\"CE_baseline\", \"OE_baseline\", \"R1_acquisition\", \"R2_acquisition\",\n",
    "                \"R3_onlineT\", \"R4_onlineT\", \"R5_onlineT\",\n",
    "                \"R6_onlineT\"]  # run IDs: CE/OE = closed/open eyes, RX = run #X\n",
    "    subA = [\"A\" + str(i) for i in range(1, 61)]  # 60 participants (Dataset A ; 29 women ; age 19-59, M = 29, SD = 9.32)\n",
    "    subB = [\"B\" + str(i) for i in\n",
    "            range(61, 82)]  # 21 participants (Dataset B ; 8 women ; age 19-37, M = 29, SD = 9.318)\n",
    "    subC = [\"C\" + str(i) for i in\n",
    "            range(82, 88)]  # 6 additional participants (Dataset C; 4 women; age 20-26, M=22; SD=2.34)\n",
    "    subjects = subA + subB + subC\n",
    "    # Note that all data followed the same EEG protocol (MI of left and right hand) with the same channel information\n",
    "\n",
    "    data = {}  # dictionary to hold all our data\n",
    "    for sub in subjects:  # for each subject...\n",
    "        skip = False\n",
    "        fnames = [sub + \"_\" + i + \".gdf\" for i in runNames]  # develop a list of all filenames\n",
    "        sub_data = []  # empty list to hold all of a subject's loaded files\n",
    "        for f in fnames:  # for each file...\n",
    "            fpath = os.path.join(path, sub, f)  # select the correct file\n",
    "            try:\n",
    "                sub_data.append(mne.io.read_raw_gdf(fpath))  # load it into the list\n",
    "            except FileNotFoundError:\n",
    "                skip = True  # if all GDF files are not available, skip this subject (for testing purposes only)\n",
    "        if not skip:\n",
    "            # correct channel type information (to properly label EOG and EMG channels)\n",
    "            new_types = []  # create a new channel types array\n",
    "            for i in sub_data:\n",
    "                for j in i.ch_names:\n",
    "                    if \"EOG\" in j:  # mark ECOG channels\n",
    "                        new_types.append(\"ecog\")\n",
    "                    elif \"EMG\" in j:  # mark EMG channels\n",
    "                        new_types.append(\"emg\")\n",
    "                    else:  # mark the rest as regular EEG\n",
    "                        new_types.append(\"eeg\")\n",
    "                i.set_channel_types(dict(zip(i.ch_names, new_types)))  # apply new channel types to raw object\n",
    "            data[sub] = sub_data  # save sub_data list into data dictionary\n",
    "\n",
    "    # Current data format: data[subject] holds all 8 raw objects\n",
    "    # data[subject][0] = Closed eyes baseline\n",
    "    # data[subject][1] = Open eyes baseline\n",
    "    # data[subject][2] = Training session 1\n",
    "    # data[subject][3] = Training session 2\n",
    "    # data[subject][4] = Test session 1 (w/ feedback)\n",
    "    # data[subject][5] = Test session 2 (w/ feedback)\n",
    "    # data[subject][6] = Test session 3 (w/ feedback)\n",
    "    # data[subject][7] = Test session 4 (w/ feedback)\n",
    "\n",
    "    if between:\n",
    "        # dic_data_format = participant number (+ _N for train) (+ _N++ for test) : mne raw object\n",
    "        dic_data = {}\n",
    "        for i in data.keys():  # for every subject...\n",
    "            for j in range(1, 7):\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data[session] = data[i][j + 1]  # place their sessions into one dictionary following indexes above\n",
    "        return data, dic_data\n",
    "    if within:\n",
    "        dic_data_train = {}\n",
    "        dic_data_test = {}\n",
    "        for i in data.keys():  # for every subject...\n",
    "            for j in range(1, 3):  # place their training sessions into one dictionary\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data_train[session] = data[i][j + 1]  # following the indexes from the comment above\n",
    "            for j in range(3, 7):  # and their testing sessions into another dictionary\n",
    "                session = str(i) + '_' + str(j)\n",
    "                dic_data_test[session] = data[i][j + 1]  # with the same indexing as the comment block above\n",
    "        return data, dic_data_train, dic_data_test\n",
    "\n",
    "\n",
    "def MS_RG_Between():\n",
    "    # load the MS dataset\n",
    "    data, dic_data = load_MS(between=True)\n",
    "\n",
    "    # run the selected pipelines\n",
    "    session = list(data.keys())  # a list of participants to be used for analysis\n",
    "    steps_preprocess = {\"filter\": [8, 30],  # filter from 8-30Hz\n",
    "                        \"drop_channels\": ['EOG1', 'EOG2', 'EOG3', 'EMGg', 'EMGd'],  # ignore EOG/EMG channels\n",
    "                        \"tmin\": 0.5, \"tmax\": 4, \"overlap\": 1/16, \"length\": 1,\n",
    "                        \"score\": \"SHAP\"}\n",
    "    accuracy = test_pipeline(dic_data, pipelines, session, steps_preprocess)  # run it!\n",
    "\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MS dataset (all subjects that are available)...\n",
      "Running a leave-one-out classification, leaving out every MS subject one at a time...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Epoching the data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n",
      "Preprocessing raw data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 984 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Instance must have 1 or 2 dimensions!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mMS_RG_Between\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [14], line 201\u001B[0m, in \u001B[0;36mMS_RG_Between\u001B[1;34m()\u001B[0m\n\u001B[0;32m    196\u001B[0m session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data\u001B[38;5;241m.\u001B[39mkeys())  \u001B[38;5;66;03m# a list of participants to be used for analysis\u001B[39;00m\n\u001B[0;32m    197\u001B[0m steps_preprocess \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilter\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m30\u001B[39m],  \u001B[38;5;66;03m# filter from 8-30Hz\u001B[39;00m\n\u001B[0;32m    198\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrop_channels\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEOG1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEOG2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEOG3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEMGg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEMGd\u001B[39m\u001B[38;5;124m'\u001B[39m],  \u001B[38;5;66;03m# ignore EOG/EMG channels\u001B[39;00m\n\u001B[0;32m    199\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtmin\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtmax\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m4\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverlap\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m16\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    200\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSHAP\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m--> 201\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtest_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdic_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpipelines\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps_preprocess\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# run it!\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m accuracy\n",
      "Cell \u001B[1;32mIn [14], line 108\u001B[0m, in \u001B[0;36mtest_pipeline\u001B[1;34m(test, pipelines, session, steps_preprocess)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m steps_preprocess[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSHAP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    107\u001B[0m     explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mKernelExplainer(pipelines[classifier]\u001B[38;5;241m.\u001B[39mpredict_proba, X_train, link\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 108\u001B[0m     shap_values_single \u001B[38;5;241m=\u001B[39m \u001B[43mexplainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshap_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnsamples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m     accuracy[classifier][subject] \u001B[38;5;241m=\u001B[39m shap_values_single\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\X-Riemannian\\venv\\lib\\site-packages\\shap\\explainers\\_kernel.py:160\u001B[0m, in \u001B[0;36mKernel.shap_values\u001B[1;34m(self, X, **kwargs)\u001B[0m\n\u001B[0;32m    158\u001B[0m     X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mtolil()\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m x_type\u001B[38;5;241m.\u001B[39mendswith(arr_type) \u001B[38;5;129;01mor\u001B[39;00m sp\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39misspmatrix_lil(X), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown instance type: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m x_type\n\u001B[1;32m--> 160\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInstance must have 1 or 2 dimensions!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;66;03m# single instance\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[1;31mAssertionError\u001B[0m: Instance must have 1 or 2 dimensions!"
     ]
    }
   ],
   "source": [
    "accuracy = MS_RG_Between()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}